# 내가 오늘 배운 것(2022-03-22)

## Deep Learning

### 복습

- `지도 학습` : Data,Label -> model 학습 -> 가중치,편향 결정 -> Data 입력 후 추론 가능

- `비지도 학습`: Data-> model 학습-> 가중치,편향 결정 -> Data 입력 후 추론 가능

1.  Data set Load 
   - Load_mnist(flatten, normalize)
     - `flatten` : 28 *28 행렬을 1\*784 행렬로 바꿈
     - `normalize` : 0~255를 0~1로 바꿈; 장점- buffer overflow 방지 및 학습 용이
   - return `train`, `test`
     - `train` : 학습시키는 데이터
     - `test` : 학습된 신경망 테스트 하는 데이터
   -  model : 신경망 완성
   -  test : test data의 output과 비교

2. 신경망 구현

   `Input(1*784)` - `Layer1(50개 뉴런)` - `Layer2(100개 뉴런)` -`outputLayer(10개)`

   - w1.shape :784*50,  b1.shape: 1\*50
   - w2.shape : 50*100, b2.shape : 1\*100
   - w3.shape : 100*10, b3.shape : 1\*10
   - w1 + b1 = a1
   - w1 = `network['w1']`
   - sigmoid(a1)=z1 
   - z1 = w2+b2 = a2
   - w2 = `network['w2']`

3.  prediction

   x_test [0], x_test[3] -> 신경망에 대입(2에서 구현); 

   prediction function 사용( `prediction(network,x)`)

   - `network` : 가중치 + 편향의 모임
   - `x` : 예측할 Data

   y = prediction(network, x_test[3])

   - y : 결과(확률)

   `np.argmax(y)`를 이용해 제일 큰 수를 뽑는다.

결과 : Accuracy(정확도)

학습의 기준 :  Loss function(Cost function) - 최소화



### 미분

- 다변수 미분

  - 미분값 : 곡선의 기울기 = 0 이되는점(극대, 극소)

  - 일변수 함수 : 극대, 극소 

  - 이변수 함수 :  y = x₁² + x₂²

    x₁, x₂

  - *다변수 함수의 미분값이 0인 포인트 : 



